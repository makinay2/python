{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c135c2b",
   "metadata": {},
   "source": [
    "# Cross validation\n",
    "\n",
    "## k-Fold Cross-Validation\n",
    "\n",
    "- Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "\n",
    "You want to evaluate the model multiple times so you can be more confident about the model design.\n",
    "\n",
    "The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n",
    "\n",
    "Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data.\n",
    "\n",
    "Note that k-fold cross-validation is to evaluate the model design, not a particular training. Because you re-trained the model of the same design with different training sets.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "    1- Shuffle the dataset randomly.\n",
    "    2- Split the dataset into k groups\n",
    "    3- For each unique group:\n",
    "        - Take the group as a hold out or test data set\n",
    "        - Take the remaining groups as a training data set\n",
    "        - Fit a model on the training set and evaluate it on the test set\n",
    "        - Retain the evaluation score and discard the model\n",
    "    4- Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.\n",
    "\n",
    "It is also important that any preparation of the data prior to fitting the model occur on the CV-assigned training dataset within the loop rather than on the broader data set. This also applies to any tuning of hyperparameters. A failure to perform these operations within the loop may result in data leakage and an optimistic estimate of the model skill.\n",
    "\n",
    "The results of a k-fold cross-validation run are often summarized with the mean of the model skill scores. It is also good practice to include a measure of the variance of the skill scores, such as the standard deviation or standard error.\n",
    "\n",
    "### Configuration of k\n",
    "\n",
    "The k value must be chosen carefully for your data sample.\n",
    "\n",
    "A poorly chosen value for k may result in a mis-representative idea of the skill of the model, such as a score with a high variance (that may change a lot based on the data used to fit the model), or a high bias, (such as an overestimate of the skill of the model).\n",
    "\n",
    "Three common tactics for choosing a value for k are as follows:\n",
    "\n",
    "    - Representative: The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.\n",
    "    - k=10: The value for k is fixed to 10, a value that has been found through experimentation to generally result in a model skill estimate with low bias a modest variance.\n",
    "    - k=n: The value for k is fixed to n, where n is the size of the dataset to give each test sample an opportunity to be used in the hold out dataset. This approach is called leave-one-out cross-validation.\n",
    "\n",
    "\n",
    "A value of k=10 is very common in the field of applied machine learning, and is recommend if you are struggling to choose a value for your dataset.\n",
    "\n",
    "If a value for k is chosen that does not evenly split the data sample, then one group will contain a remainder of the examples. It is preferable to split the data sample into k groups with the same number of samples, such that the sample of model skill scores are all equivalent.\n",
    "\n",
    "\n",
    "### Cross-Validation API\n",
    "\n",
    "The KFold() scikit-learn class can be used. It takes as arguments the number of splits, whether or not to shuffle the sample, and the seed for the pseudorandom number generator used prior to the shuffle.\n",
    "\n",
    "For example, we can create an instance that splits a dataset into 3 folds, shuffles prior to the split, and uses a value of 1 for the pseudorandom number generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b260475",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(3, True, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5452221",
   "metadata": {},
   "source": [
    "The split() function can then be called on the class where the data sample is provided as an argument. Called repeatedly, the split will return each group of train and test sets. Specifically, arrays are returned containing the indexes into the original data sample of observations to use for train and test sets on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "\tprint('train: %s, test: %s' % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb5a7c",
   "metadata": {},
   "source": [
    "We can tie all of this together with our small dataset used in the worked example of the prior section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d27a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0.1 0.2 0.3 0.6]\n",
      "Test: [0.4 0.5]\n",
      "Train: [0.2 0.4 0.5 0.6]\n",
      "Test: [0.1 0.3]\n",
      "Train: [0.1 0.3 0.4 0.5]\n",
      "Test: [0.2 0.6]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\n",
    "\n",
    "# Prepare cross-validation with 3 folds\n",
    "kfold = KFold(n_splits=3, shuffle=True)  # Use n_splits=3 for 3-fold cross-validation\n",
    "\n",
    "# Enumerate splits\n",
    "for train, test in kfold.split(data):\n",
    "    print(\"Train:\", data[train])\n",
    "    print(\"Test:\", data[test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b55090",
   "metadata": {},
   "source": [
    "- Usefully, the k-fold cross validation implementation in scikit-learn is provided as a component operation within broader methods, such as grid-searching model hyperparameters and scoring a model on a dataset.\n",
    "\n",
    "- Nevertheless, the KFold class can be used directly in order to split up a dataset prior to modeling such that all models will use the same data splits. This is especially helpful if you are working with very large data samples. The use of the same splits across algorithms can have benefits for statistical tests that you may wish to perform on the data later.\n",
    "\n",
    "### Variations on Cross-Validation\n",
    "\n",
    "There are a number of variations on the k-fold cross validation procedure.\n",
    "\n",
    "Three commonly used variations are as follows:\n",
    "\n",
    "    - Train/Test Split: Taken to one extreme, k may be set to 2 (not 1) such that a single train/test split is created to evaluate the model.\n",
    "    - LOOCV: Taken to another extreme, k may be set to the total number of observations in the dataset such that each observation is given a chance to be the held out of the dataset. This is called leave-one-out cross-validation, or LOOCV for short.\n",
    "    - Stratified: The splitting of data into folds may be governed by criteria such as ensuring that each fold has the same proportion of observations with a given categorical value, such as the class outcome value. This is called stratified cross-validation.\n",
    "    - Repeated: This is where the k-fold cross-validation procedure is repeated n times, where importantly, the data sample is shuffled prior to each repetition, which results in a different split of the sample.\n",
    "    - Nested: This is where k-fold cross-validation is performed within each fold of cross-validation, often to perform hyperparameter tuning during model evaluation. This is called nested cross-validation or double cross-validation.\n",
    "    \n",
    "### Train-Test Split for Evaluating Machine Learning Algorithms    \n",
    "\n",
    "The train-test split procedure is used to estimate the performance of machine learning algorithms when they are used to make predictions on data not used to train the model.\n",
    "\n",
    " Although simple to use and interpret, there are times when the procedure should not be used, such as when you have a small dataset and situations where additional configuration is required, such as when it is used for classification and the dataset is not balanced.\n",
    "\n",
    "#### Train-Test Split Evaluation\n",
    "\n",
    "The train-test split is a technique for evaluating the performance of a machine learning algorithm.\n",
    "\n",
    "### When to Use the Train-Test Split\n",
    "\n",
    "The idea of “sufficiently large” is specific to each predictive modeling problem. It means that there is enough data to split the dataset into train and test datasets and each of the train and test datasets are suitable representations of the problem domain. This requires that the original dataset is also a suitable representation of the problem domain.\n",
    "\n",
    "A suitable representation of the problem domain means that there are enough records to cover all common cases and most uncommon cases in the domain. This might mean combinations of input variables observed in practice. It might require thousands, hundreds of thousands, or millions of examples.\n",
    "\n",
    "Conversely, the train-test procedure is not appropriate when the dataset available is small. The reason is that when the dataset is split into train and test sets, there will not be enough data in the training dataset for the model to learn an effective mapping of inputs to outputs. There will also not be enough data in the test set to effectively evaluate the model performance. The estimated performance could be overly optimistic (good) or overly pessimistic (bad).\n",
    "\n",
    "If you have insufficient data, then a suitable alternate model evaluation procedure would be the k-fold cross-validation procedure.\n",
    "\n",
    "In addition to dataset size, another reason to use the train-test split evaluation procedure is computational efficiency.\n",
    "\n",
    "Some models are very costly to train, and in that case, repeated evaluation used in other procedures is intractable. An example might be deep neural network models. In this case, the train-test procedure is commonly used.\n",
    "\n",
    "Alternately, a project may have an efficient model and a vast dataset, although may require an estimate of model performance quickly. Again, the train-test split procedure is approached in this situation.\n",
    "\n",
    "### How to Configure the Train-Test Split\n",
    "\n",
    "You must choose a split percentage that meets your project’s objectives with considerations that include:\n",
    "\n",
    "    - Computational cost in training the model.\n",
    "    - Computational cost in evaluating the model.\n",
    "    - Training set representativeness.\n",
    "    - Test set representativeness.\n",
    "\n",
    "Nevertheless, common split percentages include:\n",
    "\n",
    "    - Train: 80%, Test: 20%\n",
    "    - Train: 67%, Test: 33%\n",
    "    - Train: 50%, Test: 50%\n",
    "    \n",
    "### Train-Test Split Procedure in Scikit-Learn\n",
    "\n",
    "The scikit-learn Python machine learning library provides an implementation of the train-test split evaluation procedure via the train_test_split() function.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc7aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a dataset into train and test sets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create dataset\n",
    "X, y = make_blobs(n_samples=1000)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160ebd3",
   "metadata": {},
   "source": [
    "- Alternatively, the dataset can be split by specifying the “train_size” argument that can be either a number of rows (integer) or a percentage of the original dataset between 0 and 1, such as 0.67 for 67 percent.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.67)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe05b5e",
   "metadata": {},
   "source": [
    "### Repeatable Train-Test Splits\n",
    "\n",
    "Another important consideration is that rows are assigned to the train and test sets randomly.\n",
    "\n",
    "This is done to ensure that datasets are a representative sample (e.g. random sample) of the original dataset, which in turn, should be a representative sample of observations from the problem domain.\n",
    "\n",
    "When comparing machine learning algorithms, it is desirable (perhaps required) that they are fit and evaluated on the same subsets of the dataset.\n",
    "\n",
    "This can be achieved by fixing the seed for the pseudo-random number generator used when splitting the dataset.\n",
    "\n",
    "- This can be achieved by setting the “random_state” to an integer value. Any value will do; it is not a tunable hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate that the train-test split procedure is repeatable\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create dataset\n",
    "X, y = make_blobs(n_samples=100)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize first 5 rows\n",
    "print(X_train[:5, :])\n",
    "# split again, and we should see the same split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "# summarize first 5 rows\n",
    "print(X_train[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75185e72",
   "metadata": {},
   "source": [
    "### Stratified Train-Test Splits\n",
    "\n",
    "One final consideration is for classification problems only.\n",
    "\n",
    "Some classification problems do not have a balanced number of examples for each class label. As such, it is desirable to split the dataset into train and test sets in a way that preserves the same proportions of examples in each class as observed in the original dataset.\n",
    "\n",
    "This is called a stratified train-test split.\n",
    "\n",
    "We can achieve this by setting the “stratify” argument to the y component of the original dataset. This will be used by the train_test_split() function to ensure that both the train and test sets have the proportion of examples in each class that is present in the provided “y” array.\n",
    "\n",
    "We can demonstrate this with an example of a classification dataset with 94 examples in one class and six examples in a second class.\n",
    "\n",
    "First, we can split the dataset into train and test sets without the “stratify” argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a306756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 94, 1: 6})\n",
      "Counter({0: 45, 1: 5})\n",
      "Counter({0: 49, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "# split imbalanced dataset into train and test sets without stratification\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=100, weights=[0.94], flip_y=0, random_state=1)\n",
    "print(Counter(y))\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1)\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7190ad",
   "metadata": {},
   "source": [
    "- Running the example first reports the composition of the dataset by class label, showing the expected 94 percent vs. 6 percent.\n",
    "\n",
    "- Then the dataset is split and the composition of the train and test sets is reported. We can see that the train set has 45/5 examples in the test set has 49/1 examples. The composition of the train and test sets differ, and this is not desirable.\n",
    "\n",
    "- Next, we can stratify the train-test split and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b3bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 94, 1: 6})\n",
      "Counter({0: 47, 1: 3})\n",
      "Counter({0: 47, 1: 3})\n"
     ]
    }
   ],
   "source": [
    "# split imbalanced dataset into train and test sets with stratification\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "# create dataset\n",
    "X, y = make_classification(n_samples=100, weights=[0.94], flip_y=0, random_state=1)\n",
    "print(Counter(y))\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=1, stratify=y)\n",
    "print(Counter(y_train))\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00eb198",
   "metadata": {},
   "source": [
    "Given that we have used a 50 percent split for the train and test sets, we would expect both the train and test sets to have 47/3 examples in the train/test sets respectively.\n",
    "\n",
    "Running the example, we can see that in this case, the stratified version of the train-test split has created both the train and test datasets with 47/3 examples in the train/test sets as we expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089571c",
   "metadata": {},
   "source": [
    "## Train-Test Split to Evaluate Machine Learning Models\n",
    "### Train-Test Split for Classification\n",
    "\n",
    "We will demonstrate how to use the train-test split to evaluate a random forest algorithm on the sonar dataset.\n",
    "\n",
    "The sonar dataset is a standard machine learning dataset composed of 208 rows of data with 60 numerical input variables and a target variable with two class values, e.g. binary classification.\n",
    "\n",
    "we can split the dataset so that 67 percent is used to train the model and 33 percent is used to evaluate it. This split was chosen arbitrarily.\n",
    "\n",
    "Then use the fit model to make predictions and evaluate the predictions using the classification accuracy performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5f4b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 60) (208,)\n",
      "(139, 60) (69, 60) (139,) (69,)\n",
      "Accuracy: 0.783\n"
     ]
    }
   ],
   "source": [
    "# train-test split evaluation random forest on the sonar dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "data = dataframe.values\n",
    "# split into inputs and outputs\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# fit the model\n",
    "model = RandomForestClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "acc = accuracy_score(y_test, yhat)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c25209",
   "metadata": {},
   "source": [
    "- Finally, the model is evaluated on the test set and the performance of the model when making predictions on new data has an accuracy of about 78.3 percent.\n",
    "\n",
    "### Train-Test Split for Regression\n",
    "\n",
    "We will demonstrate how to use the train-test split to evaluate a random forest algorithm on the housing dataset.\n",
    "\n",
    "The housing dataset is a standard machine learning dataset composed of 506 rows of data with 13 numerical input variables and a numerical target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c6dcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,)\n",
      "(339, 13) (167, 13) (339,) (167,)\n",
      "MAE: 2.171\n"
     ]
    }
   ],
   "source": [
    "# train-test split evaluation random forest on the housing dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# load dataset\n",
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "dataframe = read_csv(url, header=None)\n",
    "data = dataframe.values\n",
    "# split into inputs and outputs\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "print(X.shape, y.shape)\n",
    "# split into train test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "# fit the model\n",
    "model = RandomForestRegressor(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions\n",
    "yhat = model.predict(X_test)\n",
    "# evaluate predictions\n",
    "mae = mean_absolute_error(y_test, yhat)\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671f928",
   "metadata": {},
   "source": [
    "The dataset is split into train and test sets and we can see that there are 339 rows for training and 167 rows for the test set.\n",
    "\n",
    "Finally, the model is evaluated on the test set and the performance of the model when making predictions on new data is a mean absolute error of about 2.211 (thousands of dollars)."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
